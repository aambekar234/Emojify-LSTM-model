{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify\n",
    "## Map emoji to a text according to the context\n",
    "\n",
    "## What is covered?\n",
    "1. Data Engineering\n",
    "2. Load Embedding Vectors\n",
    "3. Train the model\n",
    "4. Test & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from util import Utils\n",
    "import numpy as np\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Label</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French macaroon is so tasty</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Doc  Label  c3    c4\n",
       "0  French macaroon is so tasty      4 NaN   NaN\n",
       "1             work is horrible      3 NaN   NaN\n",
       "2                   I am upset      3 NaN   [3]\n",
       "3               throw the ball      1 NaN   [2]\n",
       "4                    Good joke      2 NaN   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train & test csv files\n",
    "cols = [\"Doc\", \"Label\", \"c3\", \"c4\"]\n",
    "df = pd.read_csv(\"emojify_data.csv\", header=None, names = cols)\n",
    "df2 = pd.read_csv(\"test_emoji.csv\", header=None, names = cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to Emoji\n",
    "<p>The text is labeled with integers range from 0-4. Each integer corresponds to a specific emoji.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 ‚ù§Ô∏è\n",
      "label 1 ‚öæ\n",
      "label 2 üòÑ\n",
      "label 3 üòû\n",
      "label 4 üç¥\n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    \n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "#function to convert integer to printable emoji\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\n",
    "#print labels and respective emoji\n",
    "for i in range(5):\n",
    "    print(\"label\", i,label_to_emoji(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['French', 'macaroon', 'is', 'so', 'tasty'] üç¥\n",
      "['he', 'did', 'not', 'answer'] üòû\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"Doc\"]\n",
    "labels = df[\"Label\"]\n",
    "docs_test = df2[\"Doc\"]\n",
    "labels_test = df2[\"Label\"]\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "#create tokenized documents and assign labels\n",
    "for i,doc in enumerate(docs):\n",
    "    X.append(doc.split())\n",
    "    y.append(labels[i])\n",
    "    \n",
    "for i,doc in enumerate(docs_test):\n",
    "    X_test.append(doc.split())\n",
    "    y_test.append(labels_test[i])\n",
    "    \n",
    "#print first example \n",
    "print(X[0],label_to_emoji(y[0]))\n",
    "print(X_test[1],label_to_emoji(y_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GloVe Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "emb_file = 'D:\\Resources\\Glove_Embeddings\\glove.6B.50d.txt'\n",
    "dimention = 50\n",
    "word_to_index, index_to_word, word_to_vec_map = util.read_emb_vec(file_name=emb_file, dimention = dimention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#disable warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenize docs to the indices representation of glove embedding\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)                                  \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i,x in enumerate(X):\n",
    "        j = 0\n",
    "        # Loop over the tokens\n",
    "        for w in x:\n",
    "            X_indices[i, j] = word_to_index[w.lower()]\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = [['French', 'macaroon', 'is', 'so', 'tasty'], ['work', 'is', 'horrible']]\n",
      "X1_indices = [[153730. 229211. 192973. 336115. 353731.]\n",
      " [389837. 192973. 181872.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "example = [X[0],X[1]]\n",
    "example_indices = sentences_to_indices([X[0],X[1]],word_to_index, max_len = 5)\n",
    "print(\"X1 =\", example)\n",
    "print(\"X1_indices =\", example_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an embedding layer with GloVe Data for the Keras Model\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "   \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    print(emb_matrix.shape)\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras emojify LSTM Model\n",
    "def emojify_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(input=sentence_indices, output=X)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 17:13:34.781247 18676 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 17:13:34.814181 18676 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of doc is  10\n",
      "(400002, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 17:13:35.337884 18676 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0717 17:13:35.347857 18676 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0717 17:13:35.348853 18676 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0717 17:13:38.403681 18676 deprecation.py:506] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 50)            20000100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,977\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def getMaxLen(X):\n",
    "    max = 0\n",
    "    for x in X:\n",
    "        if len(x) > max:\n",
    "            max = len(x)\n",
    "    return max\n",
    "\n",
    "maxLen = getMaxLen(X)\n",
    "print(\"Max length of doc is \", maxLen)\n",
    "\n",
    "model = emojify_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X, word_to_index, maxLen)\n",
    "Y_train_oh = util.convert_to_one_hot(np.array(y), C = 5)\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "Y_test_oh = util.convert_to_one_hot(np.array(y_test), C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 17:14:44.635226 18676 deprecation.py:323] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 183 samples, validate on 56 samples\n",
      "Epoch 1/35\n",
      "183/183 [==============================] - 2s 12ms/step - loss: 1.5605 - acc: 0.2787 - val_loss: 1.4978 - val_acc: 0.4107\n",
      "Epoch 2/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 1.4929 - acc: 0.3388 - val_loss: 1.4854 - val_acc: 0.3036\n",
      "Epoch 3/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 1.4453 - acc: 0.4044 - val_loss: 1.4121 - val_acc: 0.4464\n",
      "Epoch 4/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 1.3729 - acc: 0.3989 - val_loss: 1.3115 - val_acc: 0.4821\n",
      "Epoch 5/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 1.2372 - acc: 0.5191 - val_loss: 1.1492 - val_acc: 0.5536\n",
      "Epoch 6/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 1.0258 - acc: 0.6448 - val_loss: 1.0166 - val_acc: 0.5714\n",
      "Epoch 7/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.8674 - acc: 0.6612 - val_loss: 0.9048 - val_acc: 0.6071\n",
      "Epoch 8/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.7921 - acc: 0.6721 - val_loss: 0.8459 - val_acc: 0.6071\n",
      "Epoch 9/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.6051 - acc: 0.7541 - val_loss: 0.7207 - val_acc: 0.6607\n",
      "Epoch 10/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.5230 - acc: 0.7923 - val_loss: 0.6016 - val_acc: 0.7679\n",
      "Epoch 11/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4753 - acc: 0.8033 - val_loss: 0.4705 - val_acc: 0.8393\n",
      "Epoch 12/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4387 - acc: 0.7869 - val_loss: 0.4318 - val_acc: 0.8214\n",
      "Epoch 13/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3574 - acc: 0.8798 - val_loss: 0.3821 - val_acc: 0.8571\n",
      "Epoch 14/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3313 - acc: 0.8579 - val_loss: 0.4431 - val_acc: 0.8571\n",
      "Epoch 15/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2963 - acc: 0.8798 - val_loss: 0.4263 - val_acc: 0.8393\n",
      "Epoch 16/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3563 - acc: 0.8470 - val_loss: 0.3489 - val_acc: 0.8750\n",
      "Epoch 17/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.3405 - acc: 0.8689 - val_loss: 0.2817 - val_acc: 0.9286\n",
      "Epoch 18/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3133 - acc: 0.8907 - val_loss: 0.4574 - val_acc: 0.8571\n",
      "Epoch 19/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.4923 - acc: 0.8525 - val_loss: 0.3369 - val_acc: 0.8750\n",
      "Epoch 20/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.3264 - acc: 0.8743 - val_loss: 0.2488 - val_acc: 0.9643\n",
      "Epoch 21/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2502 - acc: 0.9235 - val_loss: 0.2766 - val_acc: 0.8929\n",
      "Epoch 22/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1832 - acc: 0.9563 - val_loss: 0.2683 - val_acc: 0.8929\n",
      "Epoch 23/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1752 - acc: 0.9180 - val_loss: 0.2331 - val_acc: 0.9286\n",
      "Epoch 24/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1068 - acc: 0.9727 - val_loss: 0.1965 - val_acc: 0.9464\n",
      "Epoch 25/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1423 - acc: 0.9563 - val_loss: 0.2412 - val_acc: 0.9286\n",
      "Epoch 26/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0761 - acc: 0.9672 - val_loss: 0.1817 - val_acc: 0.9643\n",
      "Epoch 27/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.0949 - acc: 0.9672 - val_loss: 0.2041 - val_acc: 0.9464\n",
      "Epoch 28/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1130 - acc: 0.9617 - val_loss: 0.2683 - val_acc: 0.9464\n",
      "Epoch 29/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2003 - acc: 0.9454 - val_loss: 0.3677 - val_acc: 0.8750\n",
      "Epoch 30/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2204 - acc: 0.9180 - val_loss: 0.2575 - val_acc: 0.9286\n",
      "Epoch 31/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1659 - acc: 0.9180 - val_loss: 0.2344 - val_acc: 0.9464\n",
      "Epoch 32/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1998 - acc: 0.9180 - val_loss: 0.1988 - val_acc: 0.9643\n",
      "Epoch 33/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1710 - acc: 0.9235 - val_loss: 0.2115 - val_acc: 0.9643\n",
      "Epoch 34/35\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1101 - acc: 0.9672 - val_loss: 0.1922 - val_acc: 0.9643\n",
      "Epoch 35/35\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0889 - acc: 0.9727 - val_loss: 0.1630 - val_acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1198556aba8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 35, batch_size = 32, shuffle=True, validation_data=(X_test_indices, Y_test_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  I want to eat , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  he did not answer , Expected emoji:üòû, predicted üòû\n",
      "Text:  he got a raise , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  she got me a present , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  ha ha ha it was so funny , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  he is a good friend , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  I am upset , Expected emoji:‚ù§Ô∏è, predicted üòû\n",
      "Text:  We had such a lovely dinner tonight , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  where is the food , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  Stop making this joke ha ha ha , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  where is the ball , Expected emoji:‚öæ, predicted ‚öæ\n",
      "Text:  work is hard , Expected emoji:üòû, predicted üòû\n",
      "Text:  This girl is messing with me , Expected emoji:üòû, predicted üòû\n",
      "Text:  are you serious ha ha , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  Let us go play baseball , Expected emoji:‚öæ, predicted ‚öæ\n",
      "Text:  This stupid grader is not working , Expected emoji:üòû, predicted üòû\n",
      "Text:  work is horrible , Expected emoji:üòû, predicted üòû\n",
      "Text:  Congratulation for having a baby , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  stop messing around , Expected emoji:üòû, predicted üòû\n",
      "Text:  any suggestions for dinner , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  I love taking breaks , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  you brighten my day , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I boiled rice , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  she is a bully , Expected emoji:üòû, predicted üòû\n",
      "Text:  Why are you feeling bad , Expected emoji:üòû, predicted üòû\n",
      "Text:  I am upset , Expected emoji:üòû, predicted üòû\n",
      "Text:  I worked during my birthday , Expected emoji:üòû, predicted üòû\n",
      "Text:  My grandmother is the love of my life , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  enjoy your break , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  valentine day is near , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  I miss you so much , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  throw the ball , Expected emoji:‚öæ, predicted ‚öæ\n",
      "Text:  My life is so boring , Expected emoji:üòû, predicted üòû\n",
      "Text:  she said yes , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  will you be my valentine , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  he can pitch really well , Expected emoji:‚öæ, predicted ‚öæ\n",
      "Text:  dance with me , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I am starving , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  See you at the restaurant , Expected emoji:üç¥, predicted üç¥\n",
      "Text:  I like to laugh , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I will go dance , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I like your jacket , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  i miss her , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  what is your favorite baseball game , Expected emoji:‚öæ, predicted ‚öæ\n",
      "Text:  Good job , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I love to the stars and back , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  What you did was awesome , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  ha ha ha lol , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I want to joke , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  go away , Expected emoji:üòû, predicted üòû\n",
      "Text:  yesterday we lost again , Expected emoji:üòû, predicted üòû\n",
      "Text:  family is all I have , Expected emoji:‚ù§Ô∏è, predicted ‚ù§Ô∏è\n",
      "Text:  you are failing this exercise , Expected emoji:üòû, predicted üòû\n",
      "Text:  Good joke , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  You totally deserve this prize , Expected emoji:üòÑ, predicted üòÑ\n",
      "Text:  I did not have breakfast , Expected emoji:üòû, predicted üòû\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "\n",
    "Y_test = y_test\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    sentence = \" \"\n",
    "    for w in X_test[i]:\n",
    "        sentence += (w + \" \")\n",
    "    if True:\n",
    "        print('Text: ' + sentence + ', Expected emoji:' + label_to_emoji(Y_test[i]) + ', predicted ' + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
