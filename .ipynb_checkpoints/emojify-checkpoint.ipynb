{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify\n",
    "## Map emoji to a text according to the context\n",
    "\n",
    "## What is covered?\n",
    "1. Data Engineering\n",
    "2. Load Embedding Vectors\n",
    "3. Train the model\n",
    "4. Test the model\n",
    "5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from util import Utils\n",
    "import numpy as np\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Label</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French macaroon is so tasty</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Doc  Label  c3    c4\n",
       "0  French macaroon is so tasty      4 NaN   NaN\n",
       "1             work is horrible      3 NaN   NaN\n",
       "2                   I am upset      3 NaN   [3]\n",
       "3               throw the ball      1 NaN   [2]\n",
       "4                    Good joke      2 NaN   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train & test csv files\n",
    "cols = [\"Doc\", \"Label\", \"c3\", \"c4\"]\n",
    "df = pd.read_csv(\"emojify_data.csv\", header=None, names = cols)\n",
    "df2 = pd.read_csv(\"test_emoji.csv\", header=None, names = cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to Emoji\n",
    "<p>The text is labeled with integers range from 0-4. Each integer corresponds to a specific emoji.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 ‚ù§Ô∏è\n",
      "label 1 ‚öæ\n",
      "label 2 üòÑ\n",
      "label 3 üòû\n",
      "label 4 üç¥\n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    \n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"label\", i,label_to_emoji(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['French', 'macaroon', 'is', 'so', 'tasty'] üç¥\n",
      "['I', 'want', 'to', 'eat'] üç¥\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"Doc\"]\n",
    "labels = df[\"Label\"]\n",
    "docs_test = df2[\"Doc\"]\n",
    "labels_test = df2[\"Label\"]\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "\n",
    "#create tokenized documents and assign labels\n",
    "for i,doc in enumerate(docs):\n",
    "    X.append(doc.split())\n",
    "    y.append(labels[i])\n",
    "    \n",
    "for i,doc in enumerate(docs_test):\n",
    "    X_test.append(doc.split())\n",
    "    y_test.append(labels_test[i])\n",
    "    \n",
    "#print first example \n",
    "print(X[0],label_to_emoji(y[0]))\n",
    "print(X_test[0],label_to_emoji(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GloVe Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "emb_file = 'D:\\Resources\\Glove_Embeddings\\glove.6B.50d.txt'\n",
    "dimention = 50\n",
    "word_to_index, index_to_word, word_to_vec_map = util.read_emb_vec(file_name=emb_file, dimention = dimention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#disable warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenize docs to the indices representation of glove embedding\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)                                  \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i,x in enumerate(X):\n",
    "        j = 0\n",
    "        # Loop over the tokens\n",
    "        for w in x:\n",
    "            X_indices[i, j] = word_to_index[w.lower()]\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = [['French', 'macaroon', 'is', 'so', 'tasty'], ['work', 'is', 'horrible']]\n",
      "X1_indices = [[153730. 229211. 192973. 336115. 353731.]\n",
      " [389837. 192973. 181872.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "example = [X[0],X[1]]\n",
    "example_indices = sentences_to_indices([X[0],X[1]],word_to_index, max_len = 5)\n",
    "print(\"X1 =\", example)\n",
    "print(\"X1_indices =\", example_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an embedding layer with GloVe Data for the Keras Model\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "   \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    print(emb_matrix.shape)\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras emojify LSTM Model\n",
    "def emojify_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(input=sentence_indices, output=X)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 15:58:08.000837  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 15:58:08.017757  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of doc is  10\n",
      "(400002, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 15:58:08.559925  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0717 15:58:08.567904  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0717 15:58:08.569908  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0717 15:58:10.842415  5096 deprecation.py:506] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 50)            20000100  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,977\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,100\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def getMaxLen(X):\n",
    "    max = 0\n",
    "    for x in X:\n",
    "        if len(x) > max:\n",
    "            max = len(x)\n",
    "    return max\n",
    "\n",
    "maxLen = getMaxLen(X)\n",
    "print(\"Max length of doc is \", maxLen)\n",
    "\n",
    "model = emojify_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 15:58:11.068811  5096 deprecation_wrapper.py:119] From C:\\Users\\Abhijeet\\Miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X, word_to_index, maxLen)\n",
    "Y_train_oh = util.convert_to_one_hot(np.array(y), C = 5)\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "Y_test_oh = util.convert_to_one_hot(np.array(y_test), C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 183 samples, validate on 56 samples\n",
      "Epoch 1/20\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 1.0600 - acc: 0.5902 - val_loss: 1.0187 - val_acc: 0.6071\n",
      "Epoch 2/20\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.8740 - acc: 0.6557 - val_loss: 0.9849 - val_acc: 0.5714\n",
      "Epoch 3/20\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.8071 - acc: 0.6503 - val_loss: 0.7953 - val_acc: 0.6250\n",
      "Epoch 4/20\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.7006 - acc: 0.7432 - val_loss: 0.7202 - val_acc: 0.6786\n",
      "Epoch 5/20\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.5398 - acc: 0.8197 - val_loss: 0.6598 - val_acc: 0.7321\n",
      "Epoch 6/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.5203 - acc: 0.8251 - val_loss: 0.5574 - val_acc: 0.7857\n",
      "Epoch 7/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.4500 - acc: 0.8306 - val_loss: 0.4184 - val_acc: 0.8929\n",
      "Epoch 8/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.4205 - acc: 0.8470 - val_loss: 0.3946 - val_acc: 0.8571\n",
      "Epoch 9/20\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.4383 - acc: 0.8470 - val_loss: 0.4868 - val_acc: 0.7857\n",
      "Epoch 10/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3620 - acc: 0.8634 - val_loss: 0.4041 - val_acc: 0.8393\n",
      "Epoch 11/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3144 - acc: 0.8743 - val_loss: 0.4173 - val_acc: 0.8393\n",
      "Epoch 12/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.3173 - acc: 0.8798 - val_loss: 0.3254 - val_acc: 0.8929\n",
      "Epoch 13/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2627 - acc: 0.9016 - val_loss: 0.3081 - val_acc: 0.8929\n",
      "Epoch 14/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2668 - acc: 0.9180 - val_loss: 0.2904 - val_acc: 0.8750\n",
      "Epoch 15/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2212 - acc: 0.9180 - val_loss: 0.3074 - val_acc: 0.8929\n",
      "Epoch 16/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2339 - acc: 0.9235 - val_loss: 0.3722 - val_acc: 0.9107\n",
      "Epoch 17/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2710 - acc: 0.8743 - val_loss: 0.2575 - val_acc: 0.9107\n",
      "Epoch 18/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.2222 - acc: 0.9180 - val_loss: 0.2451 - val_acc: 0.9464\n",
      "Epoch 19/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1675 - acc: 0.9617 - val_loss: 0.2296 - val_acc: 0.9643\n",
      "Epoch 20/20\n",
      "183/183 [==============================] - 0s 1ms/step - loss: 0.1245 - acc: 0.9563 - val_loss: 0.2468 - val_acc: 0.9464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1daacefd898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 20, batch_size = 32, shuffle=True, validation_data=(X_test_indices, Y_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:‚ù§Ô∏è I am upset , prediction: üòû\n",
      "Expected emoji:üòû work is hard , prediction: üòÑ\n",
      "Expected emoji:üòû go away , prediction: ‚öæ\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "\n",
    "Y_test = y_test\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    sentence = \" \"\n",
    "    for w in X_test[i]:\n",
    "        sentence += (w + \" \")\n",
    "    if num != Y_test[i]:\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + sentence+ ', prediction: ' + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
